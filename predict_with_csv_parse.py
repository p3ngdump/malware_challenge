import init
from train_model import *
from glob import glob

def label_check(sha256):
    with open('E:/2020_DataChellange_train_set/label.csv', 'r') as reader:
        for line in reader:
            fields = line.strip().split(',')
            if fields[0] == sha256:
                return fields[1]

if __name__ == '__main__':
    lgbm_model = lgb.Booster(model_file="./model.txt")
    results = {
        "probability_<_0.1" : {
            "exe_count": 0,
            "is_malicious": {
                "malicious": 0,
                "not_malicious": 0,
            },
        },
        "probability_<_0.2": {
            "exe_count": 0,
            "is_malicious": {
                "malicious": 0,
                "not_malicious": 0,
            },
        },
        "probability_<_0.3": {
            "exe_count": 0,
            "is_malicious": {
                "malicious": 0,
                "not_malicious": 0,
            },
        },
        "probability_<_0.4": {
            "exe_count": 0,
            "is_malicious": {
                "malicious": 0,
                "not_malicious": 0,
            },
        },
        "probability_<_0.5": {
            "exe_count": 0,
            "is_malicious": {
                "malicious": 0,
                "not_malicious": 0,
            },
        },
        "probability_<_0.6": {
            "exe_count": 0,
            "is_malicious": {
                "malicious": 0,
                "not_malicious": 0,
            },
        },
        "probability_<_0.7": {
            "exe_count": 0,
            "is_malicious": {
                "malicious": 0,
                "not_malicious": 0,
            },
        },
        "probability_<_0.8": {
            "exe_count": 0,
            "is_malicious": {
                "malicious": 0,
                "not_malicious": 0,
            },
        },
        "probability_<_0.9": {
            "exe_count": 0,
            "is_malicious": {
                "malicious": 0,
                "not_malicious": 0,
            },
        },
        "probability_<=_1.0": {
            "exe_count": 0,
            "is_malicious": {
                "malicious": 0,
                "not_malicious": 0,
            },
        },
    }

    files = glob('E:/2020_DataChellange_train_set/*.vir')
    for i in range(len(files)):
        #if i % 1000 == 0:
        #    print(i)
        print(i)

        mal_data = open(files[i], 'rb').read()
        predict = init.predict_sample(lgbm_model, mal_data)
        sha256 = files[i].split('.')[0].split('\\')[-1]

        if predict < 0.1:
            results['probability_<_0.1']['exe_count'] += 1
            if label_check(sha256) == '1':
                results['probability_<_0.1']['is_malicious']['malicious'] += 1
            elif label_check(sha256) == '0':
                results['probability_<_0.1']['is_malicious']['not_malicious'] += 1
        elif predict < 0.2:
            results['probability_<_0.2']['exe_count'] += 1
            if label_check(sha256) == '1':
                results['probability_<_0.2']['is_malicious']['malicious'] += 1
            elif label_check(sha256) == '0':
                results['probability_<_0.2']['is_malicious']['not_malicious'] += 1
        elif predict < 0.3:
            results['probability_<_0.3']['exe_count'] += 1
            if label_check(sha256) == '1':
                results['probability_<_0.3']['is_malicious']['malicious'] += 1
            elif label_check(sha256) == '0':
                results['probability_<_0.3']['is_malicious']['not_malicious'] += 1
        elif predict < 0.4:
            results['probability_<_0.4']['exe_count'] += 1
            if label_check(sha256) == '1':
                results['probability_<_0.4']['is_malicious']['malicious'] += 1
            elif label_check(sha256) == '0':
                results['probability_<_0.4']['is_malicious']['not_malicious'] += 1
        elif predict < 0.5:
            results['probability_<_0.5']['exe_count'] += 1
            if label_check(sha256) == '1':
                results['probability_<_0.5']['is_malicious']['malicious'] += 1
            elif label_check(sha256) == '0':
                results['probability_<_0.5']['is_malicious']['not_malicious'] += 1
        elif predict < 0.6:
            results['probability_<_0.6']['exe_count'] += 1
            if label_check(sha256) == '1':
                results['probability_<_0.6']['is_malicious']['malicious'] += 1
            elif label_check(sha256) == '0':
                results['probability_<_0.6']['is_malicious']['not_malicious'] += 1
        elif predict < 0.7:
            results['probability_<_0.7']['exe_count'] += 1
            if label_check(sha256) == '1':
                results['probability_<_0.7']['is_malicious']['malicious'] += 1
            elif label_check(sha256) == '0':
                results['probability_<_0.7']['is_malicious']['not_malicious'] += 1
        elif predict < 0.8:
            results['probability_<_0.8']['exe_count'] += 1
            if label_check(sha256) == '1':
                results['probability_<_0.8']['is_malicious']['malicious'] += 1
            elif label_check(sha256) == '0':
                results['probability_<_0.8']['is_malicious']['not_malicious'] += 1
        elif predict < 0.9:
            results['probability_<_0.9']['exe_count'] += 1
            if label_check(sha256) == '1':
                results['probability_<_0.9']['is_malicious']['malicious'] += 1
            elif label_check(sha256) == '0':
                results['probability_<_0.9']['is_malicious']['not_malicious'] += 1
        elif predict <= 1.0:
            results['probability_<=_1.0']['exe_count'] += 1
            if label_check(sha256) == '1':
                results['probability_<=_1.0']['is_malicious']['malicious'] += 1
            elif label_check(sha256) == '0':
                results['probability_<=_1.0']['is_malicious']['not_malicious'] += 1

    print(results)
